{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.005728,
     "end_time": "2020-08-12T06:12:18.537618",
     "exception": false,
     "start_time": "2020-08-12T06:12:18.531890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction to the Regularized Greedy Forest (RGF)\n",
    "The RGF is a powerful technique developed by Rie Johnson and Tong Zhang in the paper [\"Learning Nonlinear Functions Using Regularized Greedy Forest\"](https://arxiv.org/pdf/1109.0887.pdf). It is on a par with gradient boosting tools like [XGBoost](https://xgboost.ai/). An ensemble of the solutions produced form these methods may well be good enough to win a kaggle competition.\n",
    "## Decision Trees\n",
    "[Decision trees](https://scikit-learn.org/stable/modules/tree.html) are perhaps one of the most venerable techniques used in machine learning, notably the [ID3](https://link.springer.com/content/pdf/10.1007/BF00116251.pdf) and **C4.5** algorithms by Ross Quinlan. \n",
    "Decision trees are simple to implement and to explain, but are prone to [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "They can be used for both classification, for example see [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), and regression, see [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html), and hence the acronym CART. \n",
    "Decision trees are what are now known as *weak learners*.\n",
    "## Ensembles and the Decision Forest\n",
    "It was shown by that one is able to create a *strong learner* from a collection, or '*ensemble*', of weak learners in a famous paper by Robert Schapire [\"The Strength of Weak Learnability\"](https://link.springer.com/content/pdf/10.1007/BF00116037.pdf). It was from this idea that came the **decision forest**, in which, as the name suggests, one by one a collection of decision trees is created. This goes by the name of [*boosting*](https://en.wikipedia.org/wiki/Boosting_&#40;machine_learning&#41;). The boosting process is what is known as being [*greedy*](https://en.wikipedia.org/wiki/Greedy_algorithm); each individual step is optimal (for example, each tree added to the forest) at the time, but this does not necessarily lead to an overall optimal solution.\n",
    "\n",
    "## Regularization\n",
    "[Regularization](https://en.wikipedia.org/wiki/Regularization_&#40;mathematics&#41;) is a technique designed to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting). In gradient boosting, an implicit regularization effect is achieved by small step size $s$ or [*shrinkage parameter*](https://en.wikipedia.org/wiki/Shrinkage_&#40;statistics&#41;), which for best results should tend to be infinitesimally small. However, as one can imagine this is not viable in practice. In the end one chooses as small an $s$ as possible, in conjunction with an *early stopping* criteria.\n",
    "In the RGF however,  an explicit regularization is used to prevent overfitting using [structured sparsity](https://en.wikipedia.org/wiki/Structured_sparsity_regularization) where the underlying forest structure is viewed as a graph of sparsity structures. In RGF one has an ensemble of forest nodes rather than individual trees.\n",
    "\n",
    "## What is RGF?\n",
    "In the words of the authors of RGF:\n",
    "\n",
    "> \"RGF integrates two ideas: one is to include tree-structured regularization into the learning formulation; and the other is to employ the  fully-corrective  regularized  greedy  algorithm.  Since  in  this  approach  we  are  able  to  take  advantage  of  the special  structure  of  the  decision  forest\"\n",
    "\n",
    "# Why use the RGF?\n",
    "The regularized greedy forest has been shown to out-perform [gradient boosting decision trees](https://en.wikipedia.org/wiki/Gradient_boosting) (GBDT), which is a technique used by [XGBoost](https://xgboost.ai/), [LightGBM](https://www.microsoft.com/en-us/research/project/lightgbm/), and [CatBoost](https://catboost.ai/). Indeed, RGF was used by the kagglers [infty](https://www.kaggle.com/infty36878) and [random modeler](https://www.kaggle.com/randommodeler) to come 1st in the kaggle [Benchmark Bond Trade Price Challenge](https://www.kaggle.com/c/benchmark-bond-trade-price-challenge) and the [Heritage Health Prize](https://www.kaggle.com/c/hhp/) competitions, and 4th place in the [Predicting a Biological Response](https://www.kaggle.com/c/bioresponse) competition.\n",
    "\n",
    "# How to use RGF:\n",
    "We shall use the [`rgf_python`](https://github.com/RGF-team/rgf/tree/master/python-package) package, written by the [RGF-team](https://github.com/RGF-team), applied first to a simple classification example; the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition data, and then to a regression example, using the [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-08-12T06:12:18.550635Z",
     "iopub.status.busy": "2020-08-12T06:12:18.549965Z",
     "iopub.status.idle": "2020-08-12T06:12:26.718752Z",
     "shell.execute_reply": "2020-08-12T06:12:26.718129Z"
    },
    "papermill": {
     "duration": 8.177245,
     "end_time": "2020-08-12T06:12:26.718890",
     "exception": false,
     "start_time": "2020-08-12T06:12:18.541645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rgf_python in /opt/conda/lib/python3.7/site-packages (3.8.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/conda/lib/python3.7/site-packages (from rgf_python) (0.23.1)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from rgf_python) (0.14.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rgf_python) (1.14.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.18->rgf_python) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.18->rgf_python) (1.18.5)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.18->rgf_python) (2.1.0)\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rgf_python"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.003219,
     "end_time": "2020-08-12T06:12:26.725924",
     "exception": false,
     "start_time": "2020-08-12T06:12:26.722705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Classification example: Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-08-12T06:12:26.744489Z",
     "iopub.status.busy": "2020-08-12T06:12:26.743447Z",
     "iopub.status.idle": "2020-08-12T06:12:27.990639Z",
     "shell.execute_reply": "2020-08-12T06:12:27.989779Z"
    },
    "papermill": {
     "duration": 1.261245,
     "end_time": "2020-08-12T06:12:27.990805",
     "exception": false,
     "start_time": "2020-08-12T06:12:26.729560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score is 0.77273\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "\n",
    "#===========================================================================\n",
    "# read in the data\n",
    "#===========================================================================\n",
    "train_data = pd.read_csv('../input/titanic/train.csv')\n",
    "test_data  = pd.read_csv('../input/titanic/test.csv')\n",
    "solution   = pd.read_csv('../input/submission-solution/submission_solution.csv')\n",
    "\n",
    "#===========================================================================\n",
    "# select some features of interest (\"ay, there's the rub\", Shakespeare)\n",
    "#===========================================================================\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "\n",
    "#===========================================================================\n",
    "# for the features that are categorical we use pd.get_dummies:\n",
    "# \"Convert categorical variable into dummy/indicator variables.\"\n",
    "#===========================================================================\n",
    "X_train       = pd.get_dummies(train_data[features])\n",
    "y_train       = train_data[\"Survived\"]\n",
    "final_X_test  = pd.get_dummies(test_data[features])\n",
    "\n",
    "#===========================================================================\n",
    "# perform the classification \n",
    "#===========================================================================\n",
    "from rgf.sklearn import RGFClassifier\n",
    "classifier = RGFClassifier(max_leaf=300, algorithm=\"RGF_Sib\", test_interval=100)\n",
    "\n",
    "#===========================================================================\n",
    "# and the fit \n",
    "#===========================================================================\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#===========================================================================\n",
    "# use the model to predict 'Survived' for the test data\n",
    "#===========================================================================\n",
    "predictions = classifier.predict(final_X_test)\n",
    "\n",
    "#===========================================================================\n",
    "# now calculate our score\n",
    "#===========================================================================\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"The score is %.5f\" % accuracy_score( solution['Survived'] , predictions ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.003296,
     "end_time": "2020-08-12T06:12:27.998266",
     "exception": false,
     "start_time": "2020-08-12T06:12:27.994970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Regression example: House Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-12T06:12:28.018093Z",
     "iopub.status.busy": "2020-08-12T06:12:28.017359Z",
     "iopub.status.idle": "2020-08-12T06:12:28.198529Z",
     "shell.execute_reply": "2020-08-12T06:12:28.197835Z"
    },
    "papermill": {
     "duration": 0.197024,
     "end_time": "2020-08-12T06:12:28.198656",
     "exception": false,
     "start_time": "2020-08-12T06:12:28.001632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score is 0.17707\n"
     ]
    }
   ],
   "source": [
    "#===========================================================================\n",
    "# read in the competition data \n",
    "#===========================================================================\n",
    "train_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test_data  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "solution   = pd.read_csv('../input/house-prices-advanced-regression-solution-file/submission.csv')\n",
    "                         \n",
    "#===========================================================================\n",
    "# select some features of interest (\"ay, there's the rub\", Shakespeare)\n",
    "#===========================================================================\n",
    "features = ['OverallQual', 'GrLivArea', 'GarageCars',  'TotalBsmtSF']\n",
    "\n",
    "#===========================================================================\n",
    "#===========================================================================\n",
    "X_train       = train_data[features]\n",
    "y_train       = train_data[\"SalePrice\"]\n",
    "final_X_test  = test_data[features]\n",
    "y_true        = solution[\"SalePrice\"]\n",
    "\n",
    "#===========================================================================\n",
    "# essential preprocessing: imputation; substitute any 'NaN' with mean value\n",
    "#===========================================================================\n",
    "X_train      = X_train.fillna(X_train.mean())\n",
    "final_X_test = final_X_test.fillna(final_X_test.mean())\n",
    "\n",
    "#===========================================================================\n",
    "# perform the regression\n",
    "#===========================================================================\n",
    "from rgf.sklearn import RGFRegressor\n",
    "regressor = RGFRegressor(max_leaf=300, algorithm=\"RGF_Sib\", test_interval=100, loss=\"LS\")\n",
    "\n",
    "#===========================================================================\n",
    "# and the fit \n",
    "#===========================================================================\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#===========================================================================\n",
    "# use the model to predict the prices for the test data\n",
    "#===========================================================================\n",
    "y_pred = regressor.predict(final_X_test)\n",
    "\n",
    "#===========================================================================\n",
    "# compare your predictions with the 'solution' using the \n",
    "# root of the mean_squared_log_error\n",
    "#===========================================================================\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "RMSLE = np.sqrt( mean_squared_log_error(y_true, y_pred) )\n",
    "print(\"The score is %.5f\" % RMSLE )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.00342,
     "end_time": "2020-08-12T06:12:28.205913",
     "exception": false,
     "start_time": "2020-08-12T06:12:28.202493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It almost goes without saying that to produce a medal winning score one does not only need a powerful estimator such as the RGF, but also perform data cleaning, judicious feature selection (perhaps using the new [Boruta-SHAP package](https://www.kaggle.com/carlmcbrideellis/feature-selection-using-borutashap)), if required then also perform  [feature engineering](http://www.feat.engineering/) as well as the necessary hyperparameter tuning and, just maybe, add a little *magic*.\n",
    "\n",
    "## RGF hyperparameters\n",
    "Here we shall mention two of the [RGF parameters](https://github.com/RGF-team/rgf/blob/master/RGF/rgf-guide.rst#432-parameters-to-control-training) that control training:\n",
    "\n",
    "`algorithm=`\n",
    "* `RGF`: RGF with $L_2$ regularization on leaf-only models. (default)\n",
    "* `RGF_Opt`: RGF with min-penalty regularization.\n",
    "* `RGF_Sib`: RGF with min-penalty regularization with the sum-to-zero sibling constraints.\n",
    "\n",
    "`loss=`\n",
    "* `LS`: square loss (default)\n",
    "* `Expo`: exponential loss\n",
    "* `Log`: logistic loss\n",
    "\n",
    "# References\n",
    "* [rgf_python](https://github.com/RGF-team/rgf/tree/master/python-package) on GitHub\n",
    "* [Regularized Greedy Forest in C++: User Guide](https://github.com/RGF-team/rgf/blob/master/RGF/rgf-guide.rst)\n",
    "* [FastRGF](https://github.com/RGF-team/rgf/tree/master/) A variant developed to be used with large (and sparse) datasets.\n",
    "* [Rie Johnson and Tong Zhang \"Learning Nonlinear Functions Using Regularized Greedy Forest\", IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume: 36 , Issue: 5  pp. 942-954 (2014)](https://dx.doi.org/10.1109/TPAMI.2013.159) ([arXiv](https://arxiv.org/abs/1109.0887))\n",
    "\n",
    "# Related reading\n",
    "* [Regularized Greedy Forest – The Scottish Play (Act I)](https://www.statworx.com/at/blog/regularized-greedy-forest-the-scottish-play-act-i/) by Fabian Müller\n",
    "* [Regularized Greedy Forest – The Scottish Play (Act II)](https://www.statworx.com/de/blog/regularized-greedy-forest-the-scottish-play-act-ii/) by Fabian Müller\n",
    "* [An Introductory Guide to Regularized Greedy Forests (RGF) with a case study in Python](https://www.analyticsvidhya.com/blog/2018/02/introductory-guide-regularized-greedy-forests-rgf-python/) by Ankit Choudhary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 14.723678,
   "end_time": "2020-08-12T06:12:28.319292",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-08-12T06:12:13.595614",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
