{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination (RFE) example\n",
    "Recursive feature elimination [1] is an example of *backward feature elimination* [2] in which we essentially first fit our model using *all* the features in a given set, then progressively one by one we remove the *least* significant features, each time re-fitting, until we are left with the desired number of features, which is set by the parameter `n_features_to_select`.\n",
    "\n",
    "This simple script uses the scikit-learn *Recursive Feature Elimination* routine [sklearn.feature_selection.RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html). In this example we shall use the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) data.\n",
    "For the regressor we shall use the [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) routine, also from scikit-learn.\n",
    "\n",
    "Scikit-learn also has a variant of this routine that incorporates [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics&#41;), see: [sklearn.feature_selection.RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html).\n",
    "\n",
    "### Forward feature selection\n",
    "RFE has its counterpart in *forward feature selection*, which does the opposite: accrete features rather than eliminate them, usually via some form of [greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm) [3]. Scikit-learn has the routine [sklearn.feature_selection.f_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) to facilitate this. \n",
    "\n",
    "Note that both of these *wrapper* methods can be beaten if one has access to \"domain knowledge\", i.e. understanding the problem and having a good idea as to which features will be important in the model one is constructing.\n",
    "\n",
    "### The python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "806c8458-34c5-4125-b015-bca5648d5f7a",
    "_uuid": "e2f3408a-218f-43bc-9df4-0ac34ce6336b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 OverallQual\n",
      "2 GrLivArea\n",
      "3 TotalBsmtSF\n",
      "4 BsmtFinSF1\n",
      "5 2ndFlrSF\n",
      "6 1stFlrSF\n",
      "7 LotArea\n",
      "8 GarageCars\n",
      "9 YearBuilt\n",
      "10 GarageArea\n",
      "11 YearRemodAdd\n",
      "12 TotRmsAbvGrd\n",
      "13 BsmtUnfSF\n",
      "14 OpenPorchSF\n",
      "15 WoodDeckSF\n",
      "16 OverallCond\n",
      "17 Fireplaces\n",
      "18 MoSold\n",
      "19 FullBath\n",
      "20 MSSubClass\n",
      "21 BedroomAbvGr\n",
      "22 YrSold\n",
      "23 BsmtFullBath\n",
      "24 KitchenAbvGr\n",
      "25 ScreenPorch\n",
      "26 EnclosedPorch\n",
      "27 BsmtFinSF2\n",
      "28 HalfBath\n",
      "29 PoolArea\n",
      "30 BsmtHalfBath\n",
      "31 3SsnPorch\n",
      "32 LowQualFinSF\n",
      "33 MiscVal\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "# coding=utf-8\n",
    "#===========================================================================\n",
    "# This is a simple script to perform recursive feature elimination on \n",
    "# the kaggle 'House Prices' data set using the scikit-learn RFE\n",
    "# Carl McBride Ellis (2.V.2020)\n",
    "#===========================================================================\n",
    "#===========================================================================\n",
    "# load up the libraries\n",
    "#===========================================================================\n",
    "import pandas  as pd\n",
    "\n",
    "#===========================================================================\n",
    "# read in the data\n",
    "#===========================================================================\n",
    "train_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test_data  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "\n",
    "#===========================================================================\n",
    "# select some features to rank. These are all 'integer' fields for today.\n",
    "#===========================================================================\n",
    "features = ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', \n",
    "        'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', \n",
    "        'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n",
    "        'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', \n",
    "        'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n",
    "        'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \n",
    "        'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', \n",
    "        'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n",
    "\n",
    "#===========================================================================\n",
    "#===========================================================================\n",
    "X_train       = train_data[features]\n",
    "y_train       = train_data[\"SalePrice\"]\n",
    "final_X_test  = test_data[features]\n",
    "\n",
    "#===========================================================================\n",
    "# simple preprocessing: imputation; substitute any 'NaN' with mean value\n",
    "#===========================================================================\n",
    "X_train      = X_train.fillna(X_train.mean())\n",
    "final_X_test = final_X_test.fillna(final_X_test.mean())\n",
    "\n",
    "#===========================================================================\n",
    "# set up our regressor. Today we shall be using the random forest\n",
    "#===========================================================================\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "\n",
    "#===========================================================================\n",
    "# perform a scikit-learn Recursive Feature Elimination (RFE)\n",
    "#===========================================================================\n",
    "from sklearn.feature_selection import RFE\n",
    "# here we want only one final feature, we do this to produce a ranking\n",
    "n_features_to_select = 1\n",
    "rfe = RFE(regressor, n_features_to_select)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "#===========================================================================\n",
    "# now print out the features in order of ranking\n",
    "#===========================================================================\n",
    "from operator import itemgetter\n",
    "for x, y in (sorted(zip(rfe.ranking_ , features), key=itemgetter(0))):\n",
    "    print(x, y)\n",
    "\n",
    "#===========================================================================\n",
    "# ok, this time let's choose the top 8 featues and use them for the model\n",
    "#===========================================================================\n",
    "n_features_to_select = 8\n",
    "rfe = RFE(regressor, n_features_to_select)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "#===========================================================================\n",
    "# use the model to predict the prices for the test data\n",
    "#===========================================================================\n",
    "predictions = rfe.predict(final_X_test)\n",
    "\n",
    "#===========================================================================\n",
    "# write out CSV submission file\n",
    "#===========================================================================\n",
    "output = pd.DataFrame({\"Id\":test_data.Id, \"SalePrice\":predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [Isabelle Guyon, Jason Weston, Stephen Barnhill and Vladimir Vapnik \"Gene Selection for Cancer Classification using Support Vector Machines\", Machine Learning volume 46, pages 389â€“422 (2002) (doi: 10.1023/A:1012487302797)](https://doi.org/10.1023/A:1012487302797)\n",
    "2. [Ron Kohavi and George H. John \"Wrappers for feature subset selection\", Artificial Intelligence Volume 97 pages 273-324 (1997) (doi: 10.1016/S0004-3702(97)00043-X)](https://www.sciencedirect.com/science/article/pii/S000437029700043X)\n",
    "3. [Haleh Vafaie and Ibrahim F. Imam \"Feature Selection Methods: Genetic Algorithms vs. Greedy-like Search\", Proceedings of the 3rd International Conference on Fuzzy and Intelligent Control Systems (1994)](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.8452)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
