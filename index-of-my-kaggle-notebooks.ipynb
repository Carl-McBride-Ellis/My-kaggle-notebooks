{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0034,
     "end_time": "2020-11-22T09:11:48.310951",
     "exception": false,
     "start_time": "2020-11-22T09:11:48.307551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Index of my kaggle notebooks\n",
    "\n",
    "## 1. data cleaning / preparation\n",
    "\n",
    "* [Titanic: view missing values with missingno](https://www.kaggle.com/carlmcbrideellis/titanic-view-missing-values-with-missingno)\n",
    "* [Data anonymization using Faker (Titanic example)](https://www.kaggle.com/carlmcbrideellis/data-anonymization-using-faker-titanic-example)\n",
    "\n",
    "## 2. exploratory data analysis\n",
    "* [Anscombe's quartet and the importance of EDA](https://www.kaggle.com/carlmcbrideellis/anscombe-s-quartet-and-the-importance-of-eda) (+ [dataset](https://www.kaggle.com/carlmcbrideellis/data-anscombes-quartet))\n",
    "* [Absolute beginners Titanic 'EDA' using dabl](https://www.kaggle.com/carlmcbrideellis/absolute-beginners-titanic-eda-using-dabl)\n",
    "* [Exploratory data analysis using pandas pivot table](https://www.kaggle.com/carlmcbrideellis/exploratory-data-analysis-using-pandas-pivot-table)\n",
    "* [House Prices: EDA in one line: 'pandas profiling'](https://www.kaggle.com/carlmcbrideellis/house-prices-eda-in-one-line-pandas-profiling)\n",
    "* Use case example: [Riiid: EDA and feature importance](https://www.kaggle.com/carlmcbrideellis/riiid-eda-and-feature-importance)\n",
    "\n",
    "\n",
    "## 3. feature selection\n",
    "* [Feature selection using the Boruta-SHAP package](https://www.kaggle.com/carlmcbrideellis/feature-selection-using-the-borutashap-package)\n",
    "* [Recursive Feature Elimination (RFE) example](https://www.kaggle.com/carlmcbrideellis/recursive-feature-elimination-rfe-example)\n",
    "* [House Prices: Permutation Importance example](https://www.kaggle.com/carlmcbrideellis/house-prices-permutation-importance-example)\n",
    "\n",
    "## 4. classification / regression\n",
    "\n",
    "This is a collection of my python example scripts for either classification, using the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition data, or regression, for which I use the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition data:\n",
    "\n",
    "| algorithm | classification | regression |\n",
    "| :--- | --- | --- |\n",
    "| baseline score | [link](https://www.kaggle.com/carlmcbrideellis/titanic-all-zeros-csv-file) | [link](https://www.kaggle.com/carlmcbrideellis/create-a-house-prices-baseline-score-0-42577) |\n",
    "| Logistic regression | [link](https://www.kaggle.com/carlmcbrideellis/logistic-regression-classifier-minimalist-script) | --- |\n",
    "| Generalized Additive Models (GAM) | [link](https://www.kaggle.com/carlmcbrideellis/classification-using-generalized-additive-models) | --- |\n",
    "| Support Vector Machines (SVM) | [link](https://www.kaggle.com/carlmcbrideellis/support-vector-classifier-minimalist-script) | --- |\n",
    "| Iterative Dichotomiser 3 (ID3) | [link](https://www.kaggle.com/carlmcbrideellis/titanic-using-the-iterative-dichotomiser-3-id3) | --- |\n",
    "| Decision tree| [link](https://www.kaggle.com/carlmcbrideellis/titanic-some-sex-a-bit-of-class-and-a-tree) | --- |\n",
    "| Random forest | [link](https://www.kaggle.com/carlmcbrideellis/random-forest-classifier-minimalist-script) | [link](https://www.kaggle.com/carlmcbrideellis/random-forest-regression-minimalist-script) |\n",
    "| Regularized Greedy Forest (RGF) | [link](https://www.kaggle.com/carlmcbrideellis/introduction-to-the-regularized-greedy-forest) | [link](https://www.kaggle.com/carlmcbrideellis/introduction-to-the-regularized-greedy-forest) |\n",
    "| Gradient boosting: XGBoost | [link](https://www.kaggle.com/carlmcbrideellis/xgboost-classification-minimalist-python-script) | [link](https://www.kaggle.com/carlmcbrideellis/very-simple-xgboost-regression)|\n",
    "| Gradient boosting: CatBoost | --- | [link](https://www.kaggle.com/carlmcbrideellis/catboost-regression-minimalist-script)|\n",
    "| Histogram gradient boosting | [link](https://www.kaggle.com/carlmcbrideellis/histogram-gradient-boosting-classifier-example) | [link](https://www.kaggle.com/carlmcbrideellis/histogram-gradient-boosting-regression-example) |\n",
    "| TabNet | [link](https://www.kaggle.com/carlmcbrideellis/tabnet-simple-binary-classification-example) | [link](https://www.kaggle.com/carlmcbrideellis/tabnet-a-very-simple-regression-example) |\n",
    "| Neural networks (using keras) | [link](https://www.kaggle.com/carlmcbrideellis/very-simple-neural-network-for-classification) | [link](https://www.kaggle.com/carlmcbrideellis/very-simple-neural-network-regression) |\n",
    "| Gaussian process | [link](https://www.kaggle.com/carlmcbrideellis/gaussian-process-classification-sample-script) | [link](https://www.kaggle.com/carlmcbrideellis/gaussian-process-regression-sample-script) |\n",
    "| Hyperparameter grid search | [link](https://www.kaggle.com/carlmcbrideellis/hyperparameter-grid-search-sample-code) | [link](https://www.kaggle.com/carlmcbrideellis/hyperparameter-grid-search-simple-example) |\n",
    "\n",
    "\n",
    "* [Automatic tuning of XGBoost with XGBTune](https://www.kaggle.com/carlmcbrideellis/automatic-tuning-of-xgboost-with-xgbtune)\n",
    "\n",
    "\n",
    "## 5. ensemble methods\n",
    "\n",
    "* [Ensemble methods: majority voting example](https://www.kaggle.com/carlmcbrideellis/ensemble-methods-majority-voting-example)\n",
    "* [ML-Ensemble example using House Prices data](https://www.kaggle.com/carlmcbrideellis/ml-ensemble-example-using-house-prices-data)\n",
    "* [Stacking ensemble using the House Prices data](https://www.kaggle.com/carlmcbrideellis/stacking-ensemble-using-the-house-prices-data)\n",
    "\n",
    "## 6. deployment\n",
    "* [All in a pickle: Saving the Titanic](https://www.kaggle.com/carlmcbrideellis/all-in-a-pickle-saving-the-titanic) - Saving our machine learning model to a file using pickle\n",
    "\n",
    "## 7. explainability\n",
    "* [KISS: Small and simple Titanic models](https://www.kaggle.com/carlmcbrideellis/kiss-small-and-simple-titanic-models)\n",
    "* [House Prices: my score using only 'OverallQual'](https://www.kaggle.com/carlmcbrideellis/house-prices-my-score-using-only-overallqual) and also a [simple two variable model](https://www.kaggle.com/carlmcbrideellis/simple-two-variable-model)\n",
    "\n",
    "## 8. didactic notebooks\n",
    "* [Beautiful math in your notebook](https://www.kaggle.com/carlmcbrideellis/beautiful-math-in-your-notebook): a guide to using $\\LaTeX$ math markup in kaggle notebooks.\n",
    "* [Titanic: In all the confusion...](https://www.kaggle.com/carlmcbrideellis/titanic-in-all-the-confusion) which looks at the confusion matrix, ROC curves, $F_1$ scores etc.\n",
    "* [Overfitting and underfitting the Titanic](https://www.kaggle.com/carlmcbrideellis/overfitting-and-underfitting-the-titanic)\n",
    "* [False positives, false negatives and the discrimination threshold](https://www.kaggle.com/carlmcbrideellis/discrimination-threshold-false-positive-negative)\n",
    "* [treeplot: See your classification tree!](https://www.kaggle.com/carlmcbrideellis/treeplot-see-your-classification-tree)\n",
    "* [Introduction to the Regularized Greedy Forest](https://www.kaggle.com/carlmcbrideellis/introduction-to-the-regularized-greedy-forest) (using [rgf_python](https://github.com/RGF-team/rgf/tree/master/python-package))\n",
    "\n",
    "\n",
    "## 9. miscellaneous\n",
    "* [Titanic leaderboard: a score > 0.8 is great!](https://www.kaggle.com/carlmcbrideellis/titanic-leaderboard-a-score-0-8-is-great)\n",
    "* [House Prices: How to work offline](https://www.kaggle.com/carlmcbrideellis/house-prices-how-to-work-offline) (+ [dataset](https://www.kaggle.com/carlmcbrideellis/house-prices-how-to-work-offline))\n",
    "* [Animated histogram of the central limit theorem](https://www.kaggle.com/carlmcbrideellis/animated-histogram-of-the-central-limit-theorem)\n",
    "\n",
    "## 10. fun with the [Meta Kaggle](https://www.kaggle.com/kaggle/meta-kaggle) dataset\n",
    "* [Notebooks: Number of views, and days, per vote](https://www.kaggle.com/carlmcbrideellis/notebooks-number-of-views-and-days-per-vote)\n",
    "* [kaggle discussions: busiest time of the day?](https://www.kaggle.com/carlmcbrideellis/kaggle-discussions-busiest-time-of-the-day) - (mentioned in [\"*Notebooks of the week: Hidden Gems*\"](https://www.kaggle.com/general/193544))\n",
    "* [The kaggle working week](https://www.kaggle.com/carlmcbrideellis/the-kaggle-working-week)\n",
    "* [WordCloud of gold medal winning notebook titles](https://www.kaggle.com/carlmcbrideellis/wordcloud-of-gold-medal-winning-notebook-titles)\n",
    "* [Shakeup interactive scatterplot maker](https://www.kaggle.com/carlmcbrideellis/shakeup-interactive-scatterplot-maker)\n",
    "* [Shakeup scatterplots: Boxes, strings and things...](https://www.kaggle.com/carlmcbrideellis/shakeup-scatterplots-boxes-strings-and-things)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 4.640282,
   "end_time": "2020-11-22T09:11:48.423290",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-22T09:11:43.783008",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
